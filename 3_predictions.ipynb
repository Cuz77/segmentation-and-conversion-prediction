{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e097bd8",
   "metadata": {},
   "source": [
    "# Part 3. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c516891",
   "metadata": {},
   "source": [
    "This a a third and final part of the project. We already analyzed and pre-processed datasets, reduced dimentions using PCA method, and clusterized population. Here, we will predict the outcome of the mailing campaign using supervised learning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "361a9e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import misc libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# import ML libraries\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d07963b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_train = pd.read_csv('dataset/mailout_train_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37881d9e",
   "metadata": {},
   "source": [
    "Before continuing with training the model, there's another concern we need to address. The class of interest is severely underrepresented.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b9b780a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    42430\n",
       "1      532\n",
       "Name: RESPONSE, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mailout_train['RESPONSE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32c48816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.987617\n",
       "1    0.012383\n",
       "Name: RESPONSE, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mailout_train['RESPONSE'].value_counts() / mailout_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881fa5ca",
   "metadata": {},
   "source": [
    "There are three ways in which I'm going ot mitigate this issue. \n",
    "- resampling techniques (by resampling the majority class)\n",
    "- evaluation metrics (selecting scoring metrics which are not overly sensitive to this problem)\n",
    "- cost-sensitive training (using balanced class weights where possible)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21694ecd",
   "metadata": {},
   "source": [
    "The most crucial part is to choose between resampling methods. To make it easier, **undersample** approach has the following effects (converse to oversampling):\n",
    "- (advantage) we will make the training process faster and less resource-intensive\n",
    "- (advantage) we're reducing the risk of overfitting\n",
    "- (advantage) we might be reducing noise from irrelevant instances of the majority class\n",
    "- (disadvantage) we might be losing some important information since the imbalance is major and we will discard many instances\n",
    "\n",
    "Unfortunately, undersampling results in a small dataset that is not sufficient to train the model. So, I've decided to oversample the minoriy class instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "156132aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mailout_train.drop(['RESPONSE'], axis=1)\n",
    "y = mailout_train['RESPONSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00d59553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampled dataset shape: (84860, 438)\n",
      "Undersampled dataset shape: (1064, 438)\n"
     ]
    }
   ],
   "source": [
    "rus = RandomUnderSampler(random_state=42)\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "X_undersampled, y_undersampled = rus.fit_resample(X, y)\n",
    "X_oversampled, y_oversampled = ros.fit_resample(X, y)\n",
    "\n",
    "print(f'Oversampled dataset shape: {X_oversampled.shape}')\n",
    "print(f'Undersampled dataset shape: {X_undersampled.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cb452c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.5\n",
       "1    0.5\n",
       "Name: RESPONSE, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_oversampled.value_counts() / y_oversampled.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafc95b8",
   "metadata": {},
   "source": [
    "We don't have to split data into training and testing datasets since these files have been provided as a part of the task. Thus, we can jump to splitting features and target variable and trying out different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4963365",
   "metadata": {},
   "source": [
    "In order to establish the best model, it's important to select a proper scoring metric. It's always a matter of some trade-offs as no single metric can tell us anything about the problem. In this instance, we could consider several of them, for instance:\n",
    "\n",
    "\n",
    "1. Precision-recall curve plots the proportion of correctly predicted positives (precision) against the proportion of all positives predicted correctly (recall). Since it's focusing on the performance of imbalanced class, it would be suitable for our case. THe scoring metrics in the **average precision** that summarizes the precision-recall curve as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight.\n",
    "\n",
    "\n",
    "2. The ROC curve plots the proportion of all positives predicted correctly (recall) against the false positive rate at various threshold settings. The final metric is the **area under the ROC curve**. It might not be the best for highly imbalanced datasets though.\n",
    "\n",
    "\n",
    "3. The F1 score measure the balance between precision and recall with the following formula:\n",
    "\n",
    "                2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    It is an improvement from simple accuracy but treats precision and recall equally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded0b99b",
   "metadata": {},
   "source": [
    "We will opt in for the ROC AU metric since we've already addressed the imbalance in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e17808ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_time(func):\n",
    "    '''\n",
    "    This is a decorator function that measures execution time of any other function.\n",
    "    '''\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # initialize time counter\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # execute finction\n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        # calculate time passed\n",
    "        end_time = time.time()\n",
    "        exec_time = end_time - start_time\n",
    "        print(f\"Function {func.__name__} took {exec_time:.1f} seconds to execute\")\n",
    "        return result\n",
    "    \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1db52a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@exec_time\n",
    "def train_clf(clf, param_grid, X, y, verbose=0):\n",
    "    '''\n",
    "    Trains a model and returns best parameters\n",
    "    \n",
    "    INPUT:\n",
    "     - clf (classfier object): classfier to train on the data\n",
    "     - param_grid (dict): dictionary with tuning parameters\n",
    "     - X (dataframe): dataframe with features\n",
    "     - y (Series): array with target class labels\n",
    "    \n",
    "    OTPUT:\n",
    "     - Best estimator for the trained model\n",
    "    '''\n",
    "    \n",
    "    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='roc_auc', cv=5, verbose=verbose)\n",
    "    grid.fit(X, y)\n",
    "    print(f'Area under curve: {grid.best_score_}\\nbest params: {grid.best_params_}')\n",
    "    \n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee43e0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000, class_weight='balanced', solver='liblinear'),\n",
    "    'RandomForestClassifier': RandomForestClassifier(class_weight='balanced'), \n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(),\n",
    "    'XGBClassifier': xgb.XGBClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ad6f1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "----------------------  Training LogisticRegression  ----------------------\n",
      "----------------------------------------------------------------------------\n",
      "\n",
      "Area under curve: 0.8159730179593467\n",
      "best params: {}\n",
      "Function train_clf took 1198.4 seconds to execute\n",
      "LogisticRegression(class_weight='balanced', max_iter=1000, solver='liblinear')\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------\n",
      "--------------------  Training RandomForestClassifier  --------------------\n",
      "----------------------------------------------------------------------------\n",
      "\n",
      "Area under curve: 0.9936583839215235\n",
      "best params: {}\n",
      "Function train_clf took 174.5 seconds to execute\n",
      "RandomForestClassifier(class_weight='balanced')\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------\n",
      "------------------  Training GradientBoostingClassifier  ------------------\n",
      "----------------------------------------------------------------------------\n",
      "\n",
      "Area under curve: 0.9046647723393967\n",
      "best params: {}\n",
      "Function train_clf took 671.8 seconds to execute\n",
      "GradientBoostingClassifier()\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------\n",
      "-------------------------  Training XGBClassifier  -------------------------\n",
      "----------------------------------------------------------------------------\n",
      "\n",
      "Area under curve: 0.9930608351396477\n",
      "best params: {}\n",
      "Function train_clf took 30.2 seconds to execute\n",
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...)\n",
      "\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# try several models\n",
    "for clf in models.keys():\n",
    "    space = (76 - len(f'  Training {clf}  ')) / 2\n",
    "    print('----------------------------------------------------------------------------')\n",
    "    print('-' * int(space) + f'  Training {clf}  ' + '-' * int(space))\n",
    "    print('----------------------------------------------------------------------------\\n')\n",
    "    print(train_clf(models[clf], {}, X_oversampled, y_oversampled))\n",
    "    print('\\n----------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dba5e4",
   "metadata": {},
   "source": [
    "Random Forest classifier and XGB classifier provide us with a shocklingly high ROC AUC strongly suggesting overfitting on the dataset. These algorithms will not generalize well on a new dataset. This is why we're choosing to pick and tune the GradientBoosting classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbd5e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'GradientBoostingClassifier': {\n",
    "        'max_depth': [5, 7],\n",
    "        'learning_rate': [0.1, 0.2],\n",
    "        'n_estimators': [100, 200]\n",
    "    }\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c015acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "------------------  Training GradientBoostingClassifier  ------------------\n",
      "----------------------------------------------------------------------------\n",
      "\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV 1/5; 1/8] START learning_rate=0.1, max_depth=5, n_estimators=100............\n"
     ]
    }
   ],
   "source": [
    "# try several tuning parameters\n",
    "for clf in models.keys():\n",
    "    space = (76 - len(f'  Training {clf}  ')) / 2\n",
    "    print('----------------------------------------------------------------------------')\n",
    "    print('-' * int(space) + f'  Training {clf}  ' + '-' * int(space))\n",
    "    print('----------------------------------------------------------------------------\\n')\n",
    "    print(train_clf(models[clf], params[clf], X_oversampled, y_oversampled, verbose=10))\n",
    "    print('\\n----------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c0f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbclf = GradientBoostingClassifier(n_estimators=60, learning_rate=0.1, max_depth=2)\n",
    "gbclf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57483ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = gbclf.predict(X)\n",
    "confusion_matrix = metrics.confusion_matrix(preds, y)\n",
    "mtx_pct = confusion_matrix / confusion_matrix.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdcbaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611e3fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtx_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40e614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.matshow(mtx_pct, cmap='copper')\n",
    "\n",
    "for i in [0, 1]:\n",
    "    for j in [0, 1]:\n",
    "        ax.text(i-0.09, j+0.02, np.round(mtx_pct[j][i], 2), color='white', size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9a6836",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'F1 score: {np.round(metrics.f1_score(preds, y_undersampled), 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81145002",
   "metadata": {},
   "source": [
    "Normally, we would test the data on the test dataset. However, the dataset for this assignment is hidden and the part of the competition is to submit the result not knowing how it performs on the test data.\n",
    "\n",
    "And with that, the project is complete. An overview of the project has been provided in the attached article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e6a136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e267979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
